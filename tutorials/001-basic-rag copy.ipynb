{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Augmentation Generation (RAG) is a technique that leverages the natural language understanding capability of large language models (llm) and more traditional techniques such as keyword search and semantic search to provide the llm knowledge about a specific topic, knowledgebase or documents that it has not been trained on. With this we can use an llm to query about our domain specific dataset/documents without training the llms. With the help of frameworks such as langchain, the implementation of a simple end to end rag solution can be done in very few lines!\n",
    "\n",
    "This tutorial shows you how to run a basic Retrieval Augmented Generation (RAG) system using langchain and openai. It follows this langchain tutorial: https://python.langchain.com/v0.2/docs/tutorials/rag/\n",
    "\n",
    "Prerequisite:\n",
    "- Follow the README instruction in the base directory.\n",
    "- Generate an api key and save it into your .env's OPENAI_API_KEY variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running and end to end rag using contents from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize your llm instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an llm using a generated api key generated from the OpenAI's web portal. The code cell below will prompt for the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=\"256\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bd2bf42c-1fcb-4d1b-9f05-e1d36f304a3e-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 9,\n",
       "  'prompt_tokens': 8,\n",
       "  'total_tokens': 17},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Data Scientist\\nDriven by a genuine passion in Artificial Intelligence, I bring a solid academic\\nfoundation with a CGPA of 4.00. As an aspiring Data Scientist, I am eager to\\nlearn and contribute my expertise in Machine Learning and Artificial\\nIntelligence to your organization. With focus of creating value through cutting-\\nedge technology, I specialize in the dynamic realm of Computer Vision, Data\\nScience and Deep Learning. Tay Xue Hao\\nPersonal Details\\nNotable Projects012-462 8936\\ntayxhwork@gmail.com\\n16, Lorong 42, Taman Petani\\nJaya, 08000 Sungai Petani,\\nKedah.\\nhttps://www.linkedin.com/in/t\\nay-xue-hao/EmailPhone\\nLinkedInAddress\\nFacial Emotion Recognition using\\nConvolutional Neural Network (Uni)\\nAutomated Cryptocurrency Trading\\nusing Deep Reinforcement Learning\\n(FYP)\\nMachine Failure Prediction (Uni)\\nEnd to end Customer Churn Analysis\\n(Personal)\\nLLM Fine-Tuning on Mental Health\\nDataset (Freelance)\\nMBTI Classification with Semi-\\nsupervised Learning (Freelance)Personal Protection Equipment\\nDetection (Jabil)\\nAOI Component Defect Detection (Jabil) \\nAXI Void/Bubble Detection (Jabil)\\nBuzzer Oritentation Detection (Jabil)https://github.com/XHTayGitHubQualifications\\n2019 - 2021 Diploma in Computer Science2021 - 2023 Bachelor of Computer Science in Data Science\\n- Tunku Abdul Rahman University College, Kuala Lumpur, Malaysia\\nAwarded a Diploma in Computer Science in May 2021. Relevant\\ncourses included Object-Oriented Programming, Database\\nDevelopment and Applications, Discrete Mathematics, and Statistics.\\nAchieved CGPA is 3.93.- Tunku Abdul Rahman University College, Kuala Lumpur, Malaysia\\nGraduated in August 2023 with a Bachelor’s Degree in Computer\\nScience, Majoring in Data Science with a CGPA of 4.00. Relevant\\nCourses include Data Science, Statistics, Artificial Intelligence, Machine\\nLearning, Image Processing and Natural Language Processing.\\nAwards\\nRegional Insurtech Data Hackathon\\nTARUC x SENHENG Talent Development ProgramAugust 2022September 2022\\nRepresented the school as the key player and leader in the Hackathon,\\nwhich included teams from the South East Asian Region on September\\n9-11, 2022. Developed a recommendation engine using both text-\\nbased data as well as images from social media. Major achievement\\nwas successfully developing an image captioning algorithm that can\\ngenerate texts from images.\\nAwarded with The Best Team Award and RM500 worth of S-Coins on\\n22 August 2022. Collaborated and conquered a plethora of\\nchallenges which tested problem solving, teamwork, and\\ncommunication skills, such as public speaking, roleplaying as\\nentrepreneurs to generate revenue, and solving conundrums.https://www.fiverr.com/draventFiverr', metadata={'source': 'c:\\\\Users\\\\USER\\\\adam-ai-poc\\\\llm-base\\\\rag\\\\../docs/TayXueHao-Resume.pdf', 'page': 0}), Document(page_content='Completed 3 different Natural language Processing gigs:\\n1. Fine-Tuning Large Language Models (LLM) with mental health  \\nq&a dataset.\\n2. Format data from raw text to JSON to prepare for fine-tuning.\\n3. Semi-supervised learning with Twitter comment dataset to\\npredict users’ MBTI personality with 20:80 labeled and unlabled\\ndata.\\nCore Skills\\nExperienced : \\nPython / Java / C++\\nMachine Learning / Deep Learning\\nData Analytics / Data Science\\nScikit Learn\\nPytorch / Tensorflow\\nPandas / Numpy\\nMatplotlib / Tableau\\nSQL / AWS S3\\nReinforcement Learning \\nGithub/Git\\nLearning :\\nDeployment / CICD pipelines\\nDocker / Flask / RestfulAPI\\nAWS ECR / EC2 / CodePipeline /\\nSagemaker\\nCommunication / Creativity\\nGenerative AI - Stable Diffusion\\nLanguageInterests\\nReading\\nGym Workout \\nSwimming\\nArtificial IntelligenceJabil Global Business Center in Bayan Lepas, PenangWork Experience\\nData Science Intern Specializing in Computer VisionFebruary 2023 - July 2023\\nPlayed a pivotal role in the development of 5 projects :\\n1. Personal Protection Equipment Detection\\nDeveloped Yolo based warning system for onsite welding workers’ safety\\ngear detection.\\n2. X-ray Void Detection \\nLed project to develop custom segmentation algorithm with Yolo, OpenCV\\nand regression analysis for PCB void detection.\\n3. AOI Machine Component Defect Detection\\nTrained and evaluated model backbones for 20k+ componenet defect\\nclassification, devised component image clustering for performance boost.\\n4. USB and PCB Defect Detection\\nDeveloped Yolo based defect detection for USB and PCB quality checks.\\n5. Buzzer Orientation Detection\\nDeveloped Yolo based model for buzzer orientation verification.September 2023 - December 2023\\nData Science Freelancer on Fiverr\\nPrincipal Data Scientist, Jabil\\nPhone: +13 817003381\\nEmail : Rita_Chen@jabil.comProgramme Leader, Tunku\\nAbdul Rahman University of\\nManagement and Technology\\nPhone: +60 12-315 6972\\nEmail : siewmooi@tarc.edu.myReferences\\nRita Chen Dr. Sandy Lim Siew MooiJanuary 2023 - Present\\nIntel Corporation\\nGraduate Trainee Focusing on AI Engineering\\nDevelop LLM RAG for internal data 1.\\nAI Compiler, model optimization. 2.\\nUpdated in April’24ExcellentExcellent\\nGood', metadata={'source': 'c:\\\\Users\\\\USER\\\\adam-ai-poc\\\\llm-base\\\\rag\\\\../docs/TayXueHao-Resume.pdf', 'page': 1})]\n"
     ]
    }
   ],
   "source": [
    "from rag.ingest import PdfIngestor\n",
    "ingestion_config = {\n",
    "        \"pdf\":{\n",
    "            \"doc_path\": \"docs/TayXueHao-Resume.pdf\",\n",
    "            \"splitter\": {\n",
    "                \"recursiveCharacterTextSplitter\": {\n",
    "                    \"chunk_size\": 200,\n",
    "                    \"chunk_overlap\": 0\n",
    "                }\n",
    "            },\n",
    "            \"embedding\":{\n",
    "                \"openAI\": {\n",
    "                    \"model_name\": \"text-embedding-3-small\",\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "}\n",
    "db_config = {\"vectordb\": \"chroma\"}\n",
    "ingestor = PdfIngestor(ingestion_config, db_config)\n",
    "vectordb = ingestor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Intel Corporation\\nGraduate Trainee Focusing on AI Engineering\\nDevelop LLM RAG for internal data 1.\\nAI Compiler, model optimization. 2.\\nUpdated in April’24ExcellentExcellent\\nGood', metadata={'page': 1, 'source': 'c:\\\\Users\\\\USER\\\\adam-ai-poc\\\\llm-base\\\\rag\\\\../docs/TayXueHao-Resume.pdf'}),\n",
       " Document(page_content='Completed 3 different Natural language Processing gigs:\\n1. Fine-Tuning Large Language Models (LLM) with mental health  \\nq&a dataset.\\n2. Format data from raw text to JSON to prepare for fine-tuning.\\n3. Semi-supervised learning with Twitter comment dataset to\\npredict users’ MBTI personality with 20:80 labeled and unlabled\\ndata.\\nCore Skills\\nExperienced : \\nPython / Java / C++\\nMachine Learning / Deep Learning\\nData Analytics / Data Science\\nScikit Learn\\nPytorch / Tensorflow\\nPandas / Numpy\\nMatplotlib / Tableau\\nSQL / AWS S3\\nReinforcement Learning \\nGithub/Git\\nLearning :\\nDeployment / CICD pipelines\\nDocker / Flask / RestfulAPI\\nAWS ECR / EC2 / CodePipeline /\\nSagemaker\\nCommunication / Creativity\\nGenerative AI - Stable Diffusion\\nLanguageInterests\\nReading\\nGym Workout \\nSwimming\\nArtificial IntelligenceJabil Global Business Center in Bayan Lepas, PenangWork Experience\\nData Science Intern Specializing in Computer VisionFebruary 2023 - July 2023\\nPlayed a pivotal role in the development of 5 projects :', metadata={'page': 1, 'source': 'c:\\\\Users\\\\USER\\\\adam-ai-poc\\\\llm-base\\\\rag\\\\../docs/TayXueHao-Resume.pdf'}),\n",
       " Document(page_content='Detection (Jabil)\\nAOI Component Defect Detection (Jabil) \\nAXI Void/Bubble Detection (Jabil)\\nBuzzer Oritentation Detection (Jabil)https://github.com/XHTayGitHubQualifications\\n2019 - 2021 Diploma in Computer Science2021 - 2023 Bachelor of Computer Science in Data Science\\n- Tunku Abdul Rahman University College, Kuala Lumpur, Malaysia\\nAwarded a Diploma in Computer Science in May 2021. Relevant\\ncourses included Object-Oriented Programming, Database\\nDevelopment and Applications, Discrete Mathematics, and Statistics.\\nAchieved CGPA is 3.93.- Tunku Abdul Rahman University College, Kuala Lumpur, Malaysia\\nGraduated in August 2023 with a Bachelor’s Degree in Computer\\nScience, Majoring in Data Science with a CGPA of 4.00. Relevant\\nCourses include Data Science, Statistics, Artificial Intelligence, Machine\\nLearning, Image Processing and Natural Language Processing.\\nAwards\\nRegional Insurtech Data Hackathon\\nTARUC x SENHENG Talent Development ProgramAugust 2022September 2022', metadata={'page': 0, 'source': 'c:\\\\Users\\\\USER\\\\adam-ai-poc\\\\llm-base\\\\rag\\\\../docs/TayXueHao-Resume.pdf'}),\n",
       " Document(page_content='1. Personal Protection Equipment Detection\\nDeveloped Yolo based warning system for onsite welding workers’ safety\\ngear detection.\\n2. X-ray Void Detection \\nLed project to develop custom segmentation algorithm with Yolo, OpenCV\\nand regression analysis for PCB void detection.\\n3. AOI Machine Component Defect Detection\\nTrained and evaluated model backbones for 20k+ componenet defect\\nclassification, devised component image clustering for performance boost.\\n4. USB and PCB Defect Detection\\nDeveloped Yolo based defect detection for USB and PCB quality checks.\\n5. Buzzer Orientation Detection\\nDeveloped Yolo based model for buzzer orientation verification.September 2023 - December 2023\\nData Science Freelancer on Fiverr\\nPrincipal Data Scientist, Jabil\\nPhone: +13 817003381\\nEmail : Rita_Chen@jabil.comProgramme Leader, Tunku\\nAbdul Rahman University of\\nManagement and Technology\\nPhone: +60 12-315 6972\\nEmail : siewmooi@tarc.edu.myReferences\\nRita Chen Dr. Sandy Lim Siew MooiJanuary 2023 - Present', metadata={'page': 1, 'source': 'c:\\\\Users\\\\USER\\\\adam-ai-poc\\\\llm-base\\\\rag\\\\../docs/TayXueHao-Resume.pdf'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.search(search_type=\"similarity\", query=\"Updates\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Hi\n",
      "Usage metadata: \n",
      "Tokens Used: 9\n",
      "\tPrompt Tokens: 8\n",
      "\tCompletion Tokens: 1\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $5.5e-06\n",
      "Response metadata: \n",
      "{\n",
      " \"token_usage\": {\n",
      "  \"completion_tokens\": 1,\n",
      "  \"prompt_tokens\": 8,\n",
      "  \"total_tokens\": 9\n",
      " },\n",
      " \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      " \"system_fingerprint\": null,\n",
      " \"finish_reason\": \"length\",\n",
      " \"logprobs\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rag.agent import OpenaiAgent\n",
    "agent = OpenaiAgent(max_tokens=1, debug=True)\n",
    "agent(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ[\"LANGCHAIN_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load our document of choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we will use langchaim's built-in webscraper to load contents from the web and store it as a Document() object which is used by langchain to perform further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary dependencies\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='IntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGInstallationHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create custom toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun LLMs locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to use a model to call toolstool_calling_parallelHow to force tool calling behaviorHow to pass tool outputs to the modelHow to pass run time values to a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystem🦜🛠️ LangSmith🦜🕸️ LangGraph🦜️🏓 LangServeVersionsOverviewRelease PolicyPackagesv0.2LangChain v0.2astream_events v2ChangesSecurityTutorialsBuild a Retrieval Augmented Generation (RAG) AppOn this pageBuild a Retrieval Augmented Generation (RAG) AppOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.This tutorial will show how to build a simple Q&A application\\nover a text data source. Along the way we’ll go over a typical Q&A\\narchitecture and highlight additional resources for more advanced Q&A techniques. We’ll also see\\nhow LangSmith can help us trace and understand our application.\\nLangSmith will become increasingly helpful as our application grows in\\ncomplexity.If you\\'re already familiar with basic retrieval, you might also be interested in\\nthis high-level overview of different retrieval techinques.What is RAG?\\u200bRAG is a technique for augmenting LLM knowledge with additional data.LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model\\'s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally. Note: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.Concepts\\u200bA typical RAG application has two main components:Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.The most common full sequence from raw data to answer looks like:Indexing\\u200bLoad: First we need to load our data. This is done with Document Loaders.Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won\\'t fit in a model\\'s finite context window.Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.Retrieval and generation\\u200bRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved dataSetup\\u200bJupyter Notebook\\u200bThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.Installation\\u200bThis tutorial requires these langchain dependencies:PipCondapip install langchain langchain_community langchain_chromaconda install langchain langchain_community langchain_chroma -c conda-forgeFor more details, see our Installation guide.LangSmith\\u200bMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.After you sign up at the link above, make sure to set your environment variables to start logging traces:export LANGCHAIN_TRACING_V2=\"true\"export LANGCHAIN_API_KEY=\"...\"Or, if in a notebook, you can set them with:import getpassimport osos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()Preview\\u200bIn this guide we’ll build a QA app over as website. The specific website we will use is the LLM Powered Autonomous\\nAgents blog post\\nby Lilian Weng, which allows us to ask questions about the contents of\\nthe post.We can create a simple indexing pipeline and RAG chain to do this in ~20\\nlines of code:OpenAIAnthropicAzureGoogleCohereFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-sonnet-20240229\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-pro\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)import bs4from langchain import hubfrom langchain_chroma import Chromafrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Load, chunk and index the contents of the blog.loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)splits = text_splitter.split_documents(docs)vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())# Retrieve and generate using the relevant snippets of the blog.retriever = vectorstore.as_retriever()prompt = hub.pull(\"rlm/rag-prompt\")def format_docs(docs):    return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)rag_chain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | prompt    | llm    | StrOutputParser())rag_chain.invoke(\"What is Task Decomposition?\")API Reference:WebBaseLoader | StrOutputParser | RunnablePassthrough | OpenAIEmbeddings | RecursiveCharacterTextSplitter\\'Task Decomposition is a process where a complex task is broken down into smaller, simpler steps or subtasks. This technique is utilized to enhance model performance on complex tasks by making them more manageable. It can be done by using language models with simple prompting, task-specific instructions, or with human inputs.\\'# cleanupvectorstore.delete_collection()Check out the LangSmith\\ntrace.Detailed walkthrough\\u200bLet’s go through the above code step-by-step to really understand what’s\\ngoing on.1. Indexing: Load\\u200bWe need to first load the blog post contents. We can use\\nDocumentLoaders\\nfor this, which are objects that load in data from a source and return a\\nlist of\\nDocuments.\\nA Document is an object with some page_content (str) and metadata\\n(dict).In this case we’ll use the\\nWebBaseLoader,\\nwhich uses urllib to load HTML from web URLs and BeautifulSoup to\\nparse it to text. We can customize the HTML -> text parsing by passing\\nin parameters to the BeautifulSoup parser via bs_kwargs (see\\nBeautifulSoup\\ndocs).\\nIn this case only HTML tags with class “post-content”, “post-title”, or\\n“post-header” are relevant, so we’ll remove all others.import bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()len(docs[0].page_content)API Reference:WebBaseLoader43131print(docs[0].page_content[:500])      LLM Powered Autonomous Agents    Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#InGo deeper\\u200bDocumentLoader: Object that loads data from a source as list of\\nDocuments.Docs:\\nDetailed documentation on how to use DocumentLoaders.Integrations: 160+\\nintegrations to choose from.Interface:\\nAPI reference \\xa0for the base interface.2. Indexing: Split\\u200bOur loaded document is over 42k characters long. This is too long to fit\\nin the context window of many models. Even for those models that could\\nfit the full post in their context window, models can struggle to find\\ninformation in very long inputs.To handle this we’ll split the Document into chunks for embedding and\\nvector storage. This should help us retrieve only the most relevant bits\\nof the blog post at run time.In this case we’ll split our documents into chunks of 1000 characters\\nwith 200 characters of overlap between chunks. The overlap helps\\nmitigate the possibility of separating a statement from important\\ncontext related to it. We use the\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.We set add_start_index=True so that the character index at which each\\nsplit Document starts within the initial Document is preserved as\\nmetadata attribute “start_index”.from langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000, chunk_overlap=200, add_start_index=True)all_splits = text_splitter.split_documents(docs)len(all_splits)API Reference:RecursiveCharacterTextSplitter66len(all_splits[0].page_content)969all_splits[10].metadata{\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 7056}Go deeper\\u200bTextSplitter: Object that splits a list of Documents into smaller\\nchunks. Subclass of DocumentTransformers.Learn more about splitting text using different methods by reading the how-to docsCode (py or js)Scientific papersInterface: API reference for the base interface.DocumentTransformer: Object that performs a transformation on a list\\nof Document objects.Docs: Detailed documentation on how to use DocumentTransformersIntegrationsInterface: API reference for the base interface.3. Indexing: Store\\u200bNow we need to index our 66 text chunks so that we can search over them\\nat runtime. The most common way to do this is to embed the contents of\\neach document split and insert these embeddings into a vector database\\n(or vector store). When we want to search over our splits, we take a\\ntext search query, embed it, and perform some sort of “similarity”\\nsearch to identify the stored splits with the most similar embeddings to\\nour query embedding. The simplest similarity measure is cosine\\nsimilarity —\\xa0we measure the cosine of the angle between each pair of\\nembeddings (which are high dimensional vectors).We can embed and store all of our document splits in a single command\\nusing the Chroma\\nvector store and\\nOpenAIEmbeddings\\nmodel.from langchain_chroma import Chromafrom langchain_openai import OpenAIEmbeddingsvectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())API Reference:OpenAIEmbeddingsGo deeper\\u200bEmbeddings: Wrapper around a text embedding model, used for converting\\ntext to embeddings.Docs: Detailed documentation on how to use embeddings.Integrations: 30+ integrations to choose from.Interface: API reference for the base interface.VectorStore: Wrapper around a vector database, used for storing and\\nquerying embeddings.Docs: Detailed documentation on how to use vector stores.Integrations: 40+ integrations to choose from.Interface: API reference for the base interface.This completes the Indexing portion of the pipeline. At this point\\nwe have a query-able vector store containing the chunked contents of our\\nblog post. Given a user question, we should ideally be able to return\\nthe snippets of the blog post that answer the question.4. Retrieval and Generation: Retrieve\\u200bNow let’s write the actual application logic. We want to create a simple\\napplication that takes a user question, searches for documents relevant\\nto that question, passes the retrieved documents and initial question to\\na model, and returns an answer.First we need to define our logic for searching over documents.\\nLangChain defines a\\nRetriever interface\\nwhich wraps an index that can return relevant Documents given a string\\nquery.The most common type of Retriever is the\\nVectorStoreRetriever,\\nwhich uses the similarity search capabilities of a vector store to\\nfacilitate retrieval. Any VectorStore can easily be turned into a\\nRetriever with VectorStore.as_retriever():retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")len(retrieved_docs)6print(retrieved_docs[0].page_content)Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Go deeper\\u200bVector stores are commonly used for retrieval, but there are other ways\\nto do retrieval, too.Retriever: An object that returns Documents given a text queryDocs: Further\\ndocumentation on the interface and built-in retrieval techniques.\\nSome of which include:MultiQueryRetriever generates variants of the input\\nquestion\\nto improve retrieval hit rate.MultiVectorRetriever instead generates\\nvariants of the\\nembeddings,\\nalso in order to improve retrieval hit rate.Max marginal relevance selects for relevance and\\ndiversity\\namong the retrieved documents to avoid passing in duplicate\\ncontext.Documents can be filtered during vector store retrieval using\\nmetadata filters, such as with a Self Query\\nRetriever.Integrations: Integrations\\nwith retrieval services.Interface:\\nAPI reference for the base interface.5. Retrieval and Generation: Generate\\u200bLet’s put it all together into a chain that takes a question, retrieves\\nrelevant documents, constructs a prompt, passes that to a model, and\\nparses the output.We’ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain LLM\\nor ChatModel could be substituted in.OpenAIAnthropicAzureGoogleCohereFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(\"model=\"claude-3-sonnet-20240229\", temperature=0.2, max_tokens=1024\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-pro\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)We’ll use a prompt for RAG that is checked into the LangChain prompt hub\\n(here).from langchain import hubprompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke(    {\"context\": \"filler context\", \"question\": \"filler question\"}).to_messages()example_messages[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\\\nQuestion: filler question \\\\nContext: filler context \\\\nAnswer:\")]print(example_messages[0].content)You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.Question: filler question Context: filler context Answer:We’ll use the LCEL Runnable\\nprotocol to define the chain, allowing us to pipe together components and functions in a transparent way automatically trace our chain in LangSmith get streaming, async, and batched calling out of the box.Here is the implementation:from langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughdef format_docs(docs):    return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)rag_chain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | prompt    | llm    | StrOutputParser())for chunk in rag_chain.stream(\"What is Task Decomposition?\"):    print(chunk, end=\"\", flush=True)API Reference:StrOutputParser | RunnablePassthroughTask Decomposition is a process where a complex task is broken down into smaller, more manageable steps or parts. This is often done using techniques like \"Chain of Thought\" or \"Tree of Thoughts\", which instruct a model to \"think step by step\" and transform large tasks into multiple simple tasks. Task decomposition can be prompted in a model, guided by task-specific instructions, or influenced by human inputs.Let\\'s dissect the LCEL to understand what\\'s going on.First: each of these components (retriever, prompt, llm, etc.) are instances of Runnable. This means that they implement the same methods-- such as sync and async .invoke, .stream, or .batch-- which makes them easier to connect together. They can be connected into a RunnableSequence-- another Runnable-- via the | operator.LangChain will automatically cast certain objects to runnables when met with the | operator. Here, format_docs is cast to a RunnableLambda, and the dict with \"context\" and \"question\" is cast to a RunnableParallel. The details are less important than the bigger point, which is that each object is a Runnable.Let\\'s trace how the input question flows through the above runnables.As we\\'ve seen above, the input to prompt is expected to be a dict with keys \"context\" and \"question\". So the first element of this chain builds runnables that will calculate both of these from the input question:retriever | format_docs passes the question through the retriever, generating Document objects, and then to format_docs to generate strings;RunnablePassthrough() passes through the input question unchanged.That is, if you constructedchain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | prompt)Then chain.invoke(question) would build a formatted prompt, ready for inference. (Note: when developing with LCEL, it can be practical to test with sub-chains like this.)The last steps of the chain are llm, which runs the inference, and StrOutputParser(), which just plucks the string content out of the LLM\\'s output message.You can analyze the individual steps of this chain via its LangSmith\\ntrace.Built-in chains\\u200bIf preferred, LangChain includes convenience functions that implement the above LCEL. We compose two functions:create_stuff_documents_chain specifies how retrieved context is fed into a prompt and LLM. In this case, we will \"stuff\" the contents into the prompt -- i.e., we will include all retrieved context without any summarization or other processing. It largely implements our above rag_chain, with input keys context and input-- it generates an answer using retrieved context and query.create_retrieval_chain adds the retrieval step and propagates the retrieved context through the chain, providing it alongside the final answer. It has input key input, and includes input, context, and answer in its output.from langchain.chains import create_retrieval_chainfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain_core.prompts import ChatPromptTemplatesystem_prompt = (    \"You are an assistant for question-answering tasks. \"    \"Use the following pieces of retrieved context to answer \"    \"the question. If you don\\'t know the answer, say that you \"    \"don\\'t know. Use three sentences maximum and keep the \"    \"answer concise.\"    \"\\\\n\\\\n\"    \"{context}\")prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system_prompt),        (\"human\", \"{input}\"),    ])question_answer_chain = create_stuff_documents_chain(llm, prompt)rag_chain = create_retrieval_chain(retriever, question_answer_chain)response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})print(response[\"answer\"])API Reference:create_retrieval_chain | create_stuff_documents_chain | ChatPromptTemplateTask Decomposition is a process in which complex tasks are broken down into smaller and simpler steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts are used to enhance model performance on these tasks. The CoT method instructs the model to think step by step, decomposing hard tasks into manageable ones, while Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thoughts.Returning sources\\u200bOften in Q&A applications it\\'s important to show users the sources that were used to generate the answer. LangChain\\'s built-in create_retrieval_chain will propagate retrieved source documents through to the output in the \"context\" key:for document in response[\"context\"]:    print(document)    print()page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 29630}Go deeper\\u200bChoosing a model\\u200bChatModel: An LLM-backed chat model. Takes in a sequence of messages\\nand returns a message.DocsIntegrations: 25+ integrations to choose from.Interface: API reference for the base interface.LLM: A text-in-text-out LLM. Takes in a string and returns a string.DocsIntegrations: 75+ integrations to choose from.Interface: API reference for the base interface.See a guide on RAG with locally-running models\\nhere.Customizing the prompt\\u200bAs shown above, we can load prompts (e.g., this RAG\\nprompt) from the prompt\\nhub. The prompt can also be easily customized:from langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)rag_chain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | custom_rag_prompt    | llm    | StrOutputParser())rag_chain.invoke(\"What is Task Decomposition?\")API Reference:PromptTemplate\\'Task decomposition is the process of breaking down a complex task into smaller, more manageable parts. Techniques like Chain of Thought (CoT) and Tree of Thoughts allow an agent to \"think step by step\" and explore multiple reasoning possibilities, respectively. This process can be executed by a Language Model with simple prompts, task-specific instructions, or human inputs. Thanks for asking!\\'Check out the LangSmith\\ntraceNext steps\\u200bWe\\'ve covered the steps to build a basic Q&A app over data:Loading data with a Document LoaderChunking the indexed data with a Text Splitter to make it more easily usable by a modelEmbedding the data and storing the data in a vectorstoreRetrieving the previously stored chunks in response to incoming questionsGenerating an answer using the retrieved chunks as contextThere’s plenty of features, integrations, and extensions to explore in each of\\nthe above sections. Along from the Go deeper sources mentioned\\nabove, good next steps include:Return sources: Learn how to return source documentsStreaming: Learn how to stream outputs and intermediate stepsAdd chat history: Learn how to add chat history to your appRetrieval conceptual guide: A high-level overview of specific retrieval techniquesEdit this pageWas this page helpful?You can also leave detailed feedback on GitHub.PreviousBuild a PDF ingestion and Question/Answering systemNextVector stores and retrieversWhat is RAG?ConceptsIndexingRetrieval and generationSetupJupyter NotebookInstallationLangSmithPreviewDetailed walkthrough1. Indexing: LoadGo deeper2. Indexing: SplitGo deeper3. Indexing: StoreGo deeper4. Retrieval and Generation: RetrieveGo deeper5. Retrieval and Generation: GenerateBuilt-in chainsGo deeperNext steps', metadata={'source': 'https://python.langchain.com/v0.2/docs/tutorials/rag/'})]\n"
     ]
    }
   ],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://python.langchain.com/v0.2/docs/tutorials/rag/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The retrieved docs is talking about LLM powered agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ingest the document and store the chunks it into a vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recursively split the document into text chunks, which will then be used to create embeddings to represent the meaning of the chunks as vectors. To create the embeddings, we will need to use an embedding model that has been trained to understand sentences and convert them into embeddings or vectors. For this demo, we use the cheapest one that OpenAI offers which is `text-embedding-3-small`.\n",
    "\n",
    "For this tutorial, we are using Chromadb as our vectordb to store the embeddings of our text chunks.\n",
    "\n",
    "More about:\n",
    "- vector db: https://www.cloudflare.com/learning/ai/what-is-vector-database/\n",
    "- chromadb: https://www.trychroma.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Initialize our retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use chromadb's as_retriever function as our retriever to retrieve relevant contexts. k=6 will retrieve top 6 most relevant context chunks to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "# Test our retriever\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(retrieved_docs)\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create our prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = PromptTemplate.from_template(\"You are a helpful assistant. Please answer based on the contexts given: {context}. Question: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Chain everything together into one rag chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our rag pipeline as a chain using Langchain Expression Language (LCEL). It involves retrieving the contexts relevant to the query, formatting the contexts, feeding the contexts into our prompt template and finally feeding the final prompt to our llm.\n",
    "\n",
    "More about LCEL: https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps, allowing an agent to better plan and execute the task effectively. It involves transforming big tasks into multiple manageable tasks, enabling a clearer interpretation of the agent's thinking process.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | system_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Use the rag chain as above with the following modifications:\n",
    "- Load a pdf document stored in docs/. \n",
    "- Reduce the chunk size and overlap the document chunks. Observe the difference.\n",
    "- Instruct the llm to answer in Chinese.\n",
    "- Experiment your own resume and have fun with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "PyPDFLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x0000021BB4C300D0>\n"
     ]
    }
   ],
   "source": [
    "print(text_splitter)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Xue Hao's CGPA is 4.00.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=\"256\", temperature=0.0)\n",
    "\n",
    "# Chunk / Ingest -> vectordb\n",
    "loader = PyPDFLoader(\"../docs/TayXueHao-Resume.pdf\")\n",
    "pages = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "# Retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 6}\n",
    "    )\n",
    "\n",
    "# Prompt templates\n",
    "system_prompt = PromptTemplate.from_template(\"You are a helpful assistant that screens resume. Please answer based on the contexts given: {context}. Question: {question}\")\n",
    "\n",
    "# Preprocessing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | system_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Query\n",
    "rag_chain.invoke(\"What is xue hao's cgpa?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurrah! You managed to chain all the pieces of rag together! Good job and have a great day!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
